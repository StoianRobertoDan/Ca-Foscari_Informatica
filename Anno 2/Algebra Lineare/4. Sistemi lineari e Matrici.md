Un sistema lineare è un sistema di equazioni.
- Ha quindi $m$ equazioni con $n$ incognite ciascuna.
	- $n$ incognite: $x_{1}, x_{2}, ..., x_{n}$.
	- Termini noti (per $m$ equazioni): $b_{1}, b_{2}, ..., b_{m}$.
		- Omogeneo se tutti i termini noti sono nulli (=0).
	- Coefficienti: $a_{mn}$
$$\left\{\begin{array}{l} a_{11}x_{1} + a_{12}x{2}+ ...+ a_{1n}x_{n} \\ a_{21}x_{1} + a_{22}x{2}+ ...+ a_{2n}x_{n} \\ ... \\ a_{m1}x_{1} + a_{m2}x{2}+ ...+ a_{mn}x_{n} \end{array} \right.$$
- Un sistema lineare può essere scritto anche come matrice (matrice incompleta):$$A=\left(\begin{array}{c} a_{11}x_{1} & a_{12}x{2}& ...& a_{1n}x_{n} \\ a_{21}x_{1} & a_{22}x{2}& ...& a_{2n}x_{n} \\ ... \\ a_{m1}x_{1} & a_{m2}x{2}& ...& a_{mn}x_{n} \end{array} \right)$$
- Un vettore può essere scritto come una matrice ad una sola colonna: $$\overline{v}= (v_{1}, ..., v_n)\in \mathbb{R} \Rightarrow \overline{v}=\left(\begin{array}{c} v_{1} \\ ... \\ v_{n} \end{array}\right)$$
	- Le incognite possono essere scritte come colonna di una matrice: $\overline{x}=\left(\begin{array}{c} x_{1} \\ ... \\ x_{m} \end{array}\right)$
	- I termini noti possono essere scritti come colonna di una matrice: $\overline{b}=\left(\begin{array}{c} b_{1} \\ ... \\ b_{m} \end{array}\right)$. Aggiungendo alla matrice incompleta la matrice dei termini noti si ha la matrice completa:$$A|b = \left(\begin{array}{c} a_{11}x_{1} & a_{12}x{2}& ...& a_{1n}x_{n} & | & b_{1} \\ a_{21}x_{1} & a_{22}x{2}& ... & a_{2n}x_{n} & | & b_{2} \\ ... & ... & ... & ... & | & ...\\ a_{m1}x_{1} & a_{m2}x{2}& ...& a_{mn}x_{n} & | & b_{m} \end{array} \right)$$
- Le matrici ($m\times n$) possono essere quadrate se $m =n$.
	- Esse hanno 2 diagonali:
		- La principale $\{a_{11}x_{1}, a_{22}x_{2}, ..., a_{mn}x_n\}$
		- La secondaria $\{a_{1n}x_{1}, a_{2n-1}x_{n-1}, ... , a_{m1}x_{m}\}$
	- Triangolazione:
		- Superiore quando i valori "sotto" la diagonale principale sono uguali a 0 ($i>j \Rightarrow a_{ij}= 0$)
		- Inferiore quando i valori "sopra" la diagonale principale sono uguali a 0 ($i<j \Rightarrow a_{ij}= 0$)
		- Caso particolare: **matrice diagonale**:
			- Quando è sia superiormente che inferiormente triangolata ($a_{ij}\neq 0 \Leftrightarrow i = j$)
			- Caso particolare: **matrice identica**:
				- Quando è diagonale e tutti i suoi valori sono 1 ($a_{ij}=1 \Leftrightarrow i = j$)
- Sottomatrice:
	- Date due matrici $A\in M_{m,n}$ e $B\in M_{r,s}$ , B è una sottomatrice di A se B è ottenuta eliminando righe e/o colonne da A con $1\leq r\leq m; 1\leq s\leq n$.
- Matrice trasposta:
	- Data una matrice $A\in M_{m,n}$ , La matrice trasposta ($A^{T}$) si ottiene scambiando le righe con le colonne.
	- Nota:
		- $\overline{v}= (v_{1}, ..., v_n)\in \mathbb{R}=\left(\begin{array}{c} v_{1} \\ ... \\ v_{n} \end{array}\right)\neq\overline{v}^{T}=\left(\begin{array}{c} v_{1} && ... && v_{n} \end{array}\right)$
	- Se $A = A^{T}$ la matrice si chiama simmetrica (valido solo per le matrici quadrate).
- Operazioni:
	- Somma:
		- Date due matrici $A, B\in M_{m,n}$ la matrice somma è $S\in M_{m,n}: s_{ij} = a_{ij}+b_{ij}\forall i,j \in m,n$
	- Moltiplicazione per scalare:
		- Data una matrice $A\in M_{m,n}$ e uno scalare $\lambda\in\mathbb{R}$ la matrice prodotto per scalare è $Ps\in M_{m,n}: ps_{ij} = \lambda a_{ij}\forall i,j \in m,n$
	- Prodotto tra matrici:
		- Date due matrici $A, B\in M_{m,n}$ la matrice prodotto è $P\in M_{m,n}: p_{ij} = \sum\limits_{k=1}^{n}a_{ik}b_{kj}\forall i,j \in m,n$
		- Ogni valore della matrice prodotto è quindi la moltiplicazione vettoriale tra la riga della prima matrice e la colonna della seconda matrice (colonna e riga che comprende la posizione di cui stiamo calcolando il valore).
	- Proprietà:
		- $A\times B \neq B\times A$
		- $A(B+C) = AB+AC$
		- $(A+B)C = AC + BC$
		- $(\lambda A)B = \lambda(AB)$
		- $(AB)C = A(BC)$
		- $A\cdot I = A = I\cdot A$ 
		- $A\cdot 0 = 0 = 0\cdot A$ 
		- $(AB)^{T}= B^{T}A^{T}$
- Matrici invertibili:
	- L'inversa della matrice $A$ è a sua volta una matrice che moltiplicata per $A$ dà come risultato la matrice identica: $A\cdot A^{-1}=I$
	- Proprietà:
		- Se $A, B$ sono invertibili, lo sono anche $A^{-1},A^{T},AB$:
			- $(A^{-1})^{-1} = A$
			- $(A^{T})^{-1}= (A^{-1})^{T}$
			- $(AB)^{-1}=A^{-1}B^{-1}$
- Omomorfismo:
	- $Hom(V,W)$ sottospazio vettoriale con $V=\mathbb{R}^{n},W=\mathbb{R}^{m}: n,m\in N$ e $M_{m,n}$
	- Teorema:
		- $Hom(\mathbb{R}^{n},\mathbb{R}^{m})\simeq M_{m,n}$ sono isomorfi
	- Corollario:
		- $Hom(\mathbb{R}^{n},\mathbb{R}^{m})\simeq M_{m,n}\simeq\mathbb{R}^{n\cdot m}$
	- es: $M_{2,2}\simeq\mathbb{R}^{4} \Rightarrow dim(M)=4$
		- Base canonica $B=\{E_{1},E_{2},E_{3},E_{4}\}$
			- $E_{1}=\left(\begin{array}{c} 1 && 0 \\ 0 && 0\end{array}\right)$ 
			- $E_{2}=\left(\begin{array}{c} 0 && 1 \\ 0 && 0\end{array}\right)$ 
			- $E_{3}=\left(\begin{array}{c} 0 && 0 \\ 1 && 0\end{array}\right)$ 
			- $E_{4}=\left(\begin{array}{c} 0 && 0 \\ 0 && 1\end{array}\right)$ 
	- $\phi:M_{m,n}\rightarrow Hom(\mathbb{R}^{n},\mathbb{R}^{m})$
		- $\phi(A)=T_{A}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
		- $A=\left(\begin{array}{c}a_{11} && ... && a_{1n}\\ ... \\ a_{m1} && ... &&a_{mn}\end{array}\right); X=\left(\begin{array}{c} x_{1}\\ ... \\ x_{n}\end{array}\right)$
		- $T_{A}(X)= AX=\left(\begin{array}{c}a_{11} && ... && a_{1n}\\ ... \\ a_{m1} && ... &&a_{mn}\end{array}\right)\left(\begin{array}{c} x_{1}\\ ... \\ x_{n}\end{array}\right)$
		- $T_{A}(x_{1}, ..., x_{n})= (a_{11}x_{1}+a_{12}x_{2}+ ... +a_{1n}x_{n}, ... ,a_{m1}x_{1}+a_{m2}x_{2}+ ... +a_{mn}x_{n})$
- Rango:
	- Massimo numero di colonne linearmente indipendenti di una matrice.
	- $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}; rg(T) = dim(Im(T))$
	- $\phi:M_{m,n}\rightarrow Hom(\mathbb{R}^{n},\mathbb{R}^{m}); rg(T_{A}) = dim(Im(T_{A}))$
	- $rg(T) = rg(T_A)$
	- Teorema:
		- $A\in M_{m,n}, rg(A) = rg(A^{T})$
		- $rg(A) \leq min\{m, n\}$
	- $rg(A) \leq rg(A|b)$
	- Kernel: **$Ker(T_{A}) = Ker(A)$**
		- $dim(Ker(A)) = 0 \Rightarrow Ker(A) = \{0\}$
		- $dim(Ker(A)) > 0\Rightarrow$ esistono altri vettori oltre quello nullo appartenenti al kernel. 
	- Teorema della dimensione:
		- $dim(Ker(A)) + rg(A) = n$
	- Teorema della struttura:
		- $W\in\mathbb{R}^{m}\rightarrow W\notin Im(T_{A})\Rightarrow T_{A}^{-1}(W) = 0$
		- $W\in Im(T_{A})\Rightarrow T_{A}^{-1}(W) = \{v_{0}+u:u\in Ker(A)\}, v_{0}\in T_{A}^{-1}(W)$
- Teorema di Rouchè-Capelli:
	- $r = rg(A); r' = rg(A|b)$
	- Il sistema lineare ammette soluzioni $\Leftrightarrow r = r'$
	- Il sistema lineare ammette una sola soluzione $\Leftrightarrow r = r' = n$
	- Se $r = r' < n$ allora il sistema lineare ammette $\infty^{n-r}$ soluzioni.
		- Infatti se $r = r' = n \Rightarrow \infty^{0}= 1$ soluzione
		- Il sistema lineare ammette infinite soluzione che dipendono dai parametri liberi.
	- Dimostrazione:
		- $v\in\mathbb{R}^{n}$ è una soluzione $\Rightarrow Av=b$
			- $T_{A}(V) = b \Rightarrow b = v_{1}A_{1} + ... + v_{n}A_{n}$
		- $r = r' \Rightarrow E =$ l'insieme delle soluzioni del sistema lineare. $E\subset\mathbb{R}^{n},E \neq 0$ 
		- $v\in E \Leftrightarrow Av=b\wedge T_{A}(v)=b\Leftrightarrow v\in T_{A}^{-1}(b); E = T_{A}^{-1}(b) \neq 0$  
		- Dal teorema della dimensione: $dim(Ker(A)) + rg(A) = n \Rightarrow dim(Ker(A)) + n = n \Rightarrow dim(Ker(A)) = 0 \Rightarrow Ker(A) = \{0\}$
		- Dal teorema della struttura: $T_{A}^{-1}(b) =\{v+u:u\in Ker(A)\} = \{v\}$ (c'è una sola soluzione)
- Algoritmo di Gauss:
	- Dati $A\in M_{n}$ una matrice quadrata e $\tilde{A}$ la matrice triangolare superiore $\Rightarrow Ax=b \Leftrightarrow \tilde{A}x=\tilde{b}$
	- Operazioni possibili su $A|b$:
		- Scambio di 2 righe.
		- Moltiplicazione di una riga per uno scalare.
		- Sostituzione di una riga con il risultato di somme e prodotti con altre righe.
	- Bisogna, colonna per colonna, azzerare tutti i coefficienti sotto la diagonale principale. Bisogna lasciare dove possibile la diagonale con coefficienti $\neq 0$.
	- Ogni riga ha quindi almeno un valore in meno rispetto alla riga precedente.
	- Il primo coefficiente non nullo di una riga si chiama **pivot**:
		- Il numero di pivot determina il rango.
		- Un sistema ammette una sola soluzione se e solo se tutti i pivot sono diversi da zero.
- Algoritmo di Gauss per sistemi lineari non quadrati:
	- Si utilizzano le matrici a scala invece che le matrici triangolari superiori.
	- Se il sistema lineare è a scala con $p$ i **pivot** e $p_{r}$ l'ultimo pivot, allora $rg(S) = r$
	- Le colonne che contengono pivot sono dette **colonne dominanti**.
	- Proprietà:
		- $Ax=b$ e $Sx=c$ (la matrice A in forma a scala) sono equivalenti.
		- $Ker(A) = Ker(S)$ ($Ker(A) = Ker(T_{A})$ e $T_{A}(x)=Ax$)
		- $rg(A) = rg(S)$
		- Una base del sottospazio vettoriale $Im(A)$ ($=Im(T_{A})$) è data dalle colonne $A_{j1},..., A_{jk}$ (le colonne dominanti).
- Determinante:
	- Proprietà:
		- Se $A$ ha una riga nulla allora $det(A) = 0$
		- $d(A)$ non cambia se si applicano regole di Gauss.
		- $d(A)$ cambia segno se ad ogni scambio di righe.
		- Se $S$ è la matrice triangolare superiore ottenuta da $A$ con l'argomento con $k$ scambi di righe:
			- $d(S)=(-1)^{k}d(A)$
		- Se le righe di $A$ sono linearmente dipendenti $d(A)$
		- Teorema di Binet:
			- $det(A\cdot B)=det(A)\cdot det(B)$
	- Teorema di Sarrus:
		- Valido per matrici $2\times 2$ e $3\times 3$ e consiste nella differenza tra le diagonali principali e quelle inverse.
	- Teorema di Laplace:
		- Valido anche per matrici più grandi.
		- Complemento algebrico:
			- $(-1)^{i+j}\cdot det(A_{ij})$
			- Dove $det(A_{ij})$ è la sottomatrice di $A$ senza la riga $i$ e la colonna $j$.
		- Sviluppo di Laplace per righe:
			- $det(A) = \sum\limits_{j=1}^{n}[a_{ij}\cdot (-1)^{i+j}\cdot det(A_{ij})]$
		- Sviluppo di Laplace per colonne:
			- $det(A) = \sum\limits_{i=1}^{n}[a_{ij}\cdot (-1)^{i+j}\cdot det(A_{ij})]$
	- Teorema di Gauss:
		- valido anche per matrici più grandi.
		- Si triangolarizza una matrice e si calcola il prodotto degli elementi non nulli della diagonale principale.
- Matrice inversa:
	- Una matrice è inversa se e solo se è non singolare (e quindi la sua determinante è diversa da $0$).
	- 